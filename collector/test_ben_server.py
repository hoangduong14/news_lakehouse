# -*- coding: utf-8 -*-
import json
import logging
import re
import os
import time
import uuid
import unicodedata
import threading
import requests
from datetime import datetime, date, timedelta, timezone
from typing import Dict, List, Optional, Set, Tuple
from urllib.parse import urljoin, urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException, WebDriverException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.remote.remote_connection import RemoteConnection



import tempfile, shutil

_thread_local = threading.local()

from reaction_map import REACTION_MAP

# ======================== LOGGING ========================
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger("NewsCrawlerVNE_All")

# ======================== OPTIONS ========================
USE_INGESTED_DATE_FALLBACK = False
MAX_WORKERS = int(os.getenv("MAX_WORKERS", "1"))
HTTP_TIMEOUT = (5, 15)
HTTP_RETRIES = 3
PER_PAGE_SLEEP = 0.6

# Comments
ENABLE_COMMENTS = os.getenv("ENABLE_COMMENTS", "true").lower() == "true"
COMMENT_WORKERS = int(os.getenv("COMMENT_WORKERS", "2"))
COMMENT_LIMIT = int(os.getenv("COMMENT_LIMIT", "50"))
COMMENT_WAIT = int(os.getenv("COMMENT_WAIT", "6"))

# GCS


GCS_BUCKET_NAME = os.getenv("GCS_BUCKET_NAME", "my-crawl-bucket-bronze")
GCS_CREATE_IF_NOT_EXISTS = os.getenv("GCS_CREATE_IF_NOT_EXISTS", "true").lower() == "true"
GCP_PROJECT_ID = os.getenv("GOOGLE_CLOUD_PROJECT")

# --- START: explicit service-account credentials (paste here) ---
import os
from google.oauth2 import service_account
from google.cloud import storage

#Tren Server
# SA_PATH = r"C:\Users\Administrator\Documents\DA2\mythical-bazaar-475215-i7-337ee07f878c.json"
# PROJECT_ID = "mythical-bazaar-475215-i7"

SA_PATH = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
PROJECT_ID = os.getenv("GOOGLE_CLOUD_PROJECT", "mythical-bazaar-475215-i7")

if not os.path.exists(SA_PATH):
    raise RuntimeError(f"Service account JSON not found: {SA_PATH}")

creds = service_account.Credentials.from_service_account_file(SA_PATH)
storage_client = storage.Client(project=PROJECT_ID, credentials=creds)
# --- END ---

# storage_client = storage.Client(project=GCP_PROJECT_ID)

def _get_or_create_bucket(bucket_name: str):
    bucket = storage_client.lookup_bucket(bucket_name)
    if bucket:
        return bucket
    if not GCS_CREATE_IF_NOT_EXISTS:
        raise RuntimeError(f"GCS bucket '{bucket_name}' kh√¥ng t·ªìn t·∫°i.")
    if not (GCP_PROJECT_ID or storage_client.project):
        raise RuntimeError("Thi·∫øu GOOGLE_CLOUD_PROJECT ƒë·ªÉ t·∫°o bucket.")
    bucket = storage_client.create_bucket(storage_client.bucket(bucket_name))
    logger.info(f"Created GCS bucket: {bucket.name}")
    return bucket

_gcs_bucket = _get_or_create_bucket(GCS_BUCKET_NAME)

def save_to_gcs(data_bytes: bytes, object_name: str, retries: int = 3):
    blob = _gcs_bucket.blob(object_name)
    for i in range(retries):
        try:
            blob.upload_from_string(data_bytes, content_type="application/json; charset=utf-8")
            logger.info(f"Uploaded to GCS: gs://{GCS_BUCKET_NAME}/{object_name}")
            return
        except Exception as e:
            logger.warning(f"GCS upload failed ({i+1}/{retries}): {e}")
            time.sleep(2)
    logger.error(f"Failed to upload after {retries} retries: {object_name}")

def generate_file_name():
    return f"{uuid.uuid4().hex}.json"

def slugify_topic(topic: str) -> str:
    s = (topic or "").lower().strip()
    s = s.replace("ƒë", "d")
    s = unicodedata.normalize("NFD", s)
    s = "".join(ch for ch in s if unicodedata.category(ch) != "Mn")
    s = re.sub(r"[^a-z0-9]+", "-", s)
    s = re.sub(r"-{2,}", "-", s).strip("-")
    return s or "unknown"

def build_object_path(topic: str, year: int = None, month: int = None, day: int = None, prefix: str = "") -> str:
    safe_topic = slugify_topic(topic)
    fname = prefix + generate_file_name()
    if year and month and day:
        return f"vnexpress/{safe_topic}/{year:04d}/{month:02d}/{day:02d}/{fname}"
    return f"vnexpress/{safe_topic}/undated/{fname}"

# ======================== TIMEZONE ========================
try:
    from zoneinfo import ZoneInfo
    VN_TZ = ZoneInfo("Asia/Ho_Chi_Minh")
except Exception:
    VN_TZ = None

def now_vn():
    if VN_TZ:
        return datetime.now(VN_TZ)
    return datetime.now(timezone.utc) + timedelta(hours=7)

def _dt_from_iso_local(iso_str: str) -> Optional[datetime]:
    if not iso_str:
        return None
    s = iso_str.strip().replace("Z", "")
    try:
        dt = datetime.fromisoformat(s)
        if VN_TZ:
            if dt.tzinfo is None:
                return dt.replace(tzinfo=VN_TZ)
            return dt.astimezone(VN_TZ)
        return dt
    except Exception:
        return None

# ======================== CATEIDS ========================
CATEIDS = {
    "Th·ªùi s·ª±": "1001005",
    "G√≥c nh√¨n": "1003450",
    "Th·∫ø gi·ªõi": "1001002",
    "Kinh doanh": "1003159",
    "B·∫•t ƒë·ªông s·∫£n": "1002565",
    "Gi√°o d·ª•c": "1003497",
    "Ph√°p lu·∫≠t": "1001007",
    "S·ª©c kh·ªèe": "1003750",
    "ƒê·ªùi s·ªëng": "1002966",
    "Du l·ªãch": "1003231",
    "Khoa h·ªçc": "1006219",
}

EXCLUDE_PATTERNS = (
    r"/interactive/", r"/video/", r"/podcast/", r"/photo/", r"/anh/",
    r"/infographic", r"/tra-cuu", r"/tracuu", r"/lien-he", r"/contact",
    r"/tin-tuc-anh", r"/multimedia", r"/#",
)
exclude_re = re.compile("|".join(EXCLUDE_PATTERNS), re.IGNORECASE)

# ======================== SELENIUM (Thread-local) ========================
_thread_local = threading.local()

def get_driver():
    if not hasattr(_thread_local, "driver"):
        UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36"
        options = Options()
        options.add_argument("--headless=new")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument(f"--user-agent={UA}")
        options.set_capability("pageLoadStrategy", "eager")

        # T·∫ÆT ·∫£nh, font, css ƒë·ªÉ ƒë·ª° t·ªën RAM/bandwidth
        prefs = {
            "profile.managed_default_content_settings.images": 2,
            "profile.managed_default_content_settings.stylesheets": 2,
            "profile.managed_default_content_settings.fonts": 2,
            "profile.managed_default_content_settings.cookies": 1,
            "profile.managed_default_content_settings.javascript": 1,  # JS v·∫´n b·∫≠t (c·∫ßn cho comment)
        }
        options.add_experimental_option("prefs", prefs)

        # T·∫°o th∆∞ m·ª•c t·∫°m cho profile Chrome
        profile_dir = tempfile.mkdtemp(prefix="vne-selenium-")
        options.add_argument(f"--user-data-dir={profile_dir}")

        driver = webdriver.Chrome(options=options)

        # timeout load trang (page_load_timeout) ‚Äì ng·∫Øn h∆°n ƒë·ªÉ kh√¥ng ch·ªù qu√° l√¢u
        driver.set_page_load_timeout(30)
        driver.implicitly_wait(0)

        # üî• GI·∫¢M TIMEOUT CHO M·ªñI L·ªÜNH SELENIUM & T·∫ÆT RETRY NG·∫¶M C·ª¶A urllib3
        try:
            # M·∫∑c ƒë·ªãnh l√† 120s; m√¨nh h·∫° xu·ªëng 30s
            driver.command_executor.set_timeout(30)
        except Exception as e:
            logger.warning(f"Kh√¥ng set timeout cho command_executor ƒë∆∞·ª£c: {e}")

        try:
            # T·∫Øt Retry(total=3, ...) c·ªßa urllib3 tr√™n k·∫øt n·ªëi localhost t·ªõi chromedriver
            conn = getattr(driver.command_executor, "_conn", None)
            if conn and hasattr(conn, "adapters"):
                for prefix in ("http://", "https://"):
                    ad = conn.adapters.get(prefix)
                    if ad and hasattr(ad, "max_retries"):
                        ad.max_retries = 0
        except Exception as e:
            logger.warning(f"Kh√¥ng ch·ªânh ƒë∆∞·ª£c max_retries urllib3: {e}")

        _thread_local.driver = driver
        _thread_local.profile_dir = profile_dir

    return _thread_local.driver


def cleanup_driver():
    """ƒê√≥ng Chrome v√† x√≥a th∆∞ m·ª•c t·∫°m."""
    driver = getattr(_thread_local, "driver", None)
    profile_dir = getattr(_thread_local, "profile_dir", None)

    if driver is not None:
        try:
            driver.quit()
        except Exception as e:
            logger.warning(f"L·ªói khi quit driver: {e}")

    if profile_dir and os.path.exists(profile_dir):
        try:
            shutil.rmtree(profile_dir, ignore_errors=True)
            logger.info(f"ƒê√£ x√≥a th∆∞ m·ª•c t·∫°m: {profile_dir}")
        except Exception as e:
            logger.warning(f"L·ªói khi x√≥a th∆∞ m·ª•c t·∫°m {profile_dir}: {e}")

    # X√≥a thu·ªôc t√≠nh ƒë·ªÉ tr√°nh reuse driver ƒë√£ h·ªèng
    if hasattr(_thread_local, "driver"):
        del _thread_local.driver
    if hasattr(_thread_local, "profile_dir"):
        del _thread_local.profile_dir

import atexit
atexit.register(cleanup_driver)


def safe_get(driver, url, wait_css=None, timeout=25, max_attempts=2) -> bool:
    for attempt in range(1, max_attempts + 1):
        try:
            logger.info(f"Attempt {attempt}/{max_attempts} to load: {url}")
            driver.set_page_load_timeout(timeout)
            driver.get(url)

            if wait_css:
                try:
                    WebDriverWait(driver, timeout).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, wait_css))
                    )
                except TimeoutException:
                    logger.warning(
                        f"wait_css timeout (kh√¥ng t√¨m th·∫•y '{wait_css}') ‚Äì v·∫´n d√πng page_source hi·ªán t·∫°i: {url}"
                    )

            return True

        except Exception as e:
            msg = str(e)
            logger.warning(f"safe_get: attempt {attempt} l·ªói cho {url}: {msg}")

            # ‚ùóN·∫øu KH√îNG ph·∫£i l·ªói "timed out" ‚Üí b·ªè lu√¥n
            if "timed out" not in msg.lower():
                logger.error(f"safe_get: l·ªói kh√¥ng ph·∫£i timeout -> b·ªè lu√¥n url n√†y: {url}")
                return False

            # ‚ùóTimeout ƒë·ªß s·ªë l·∫ßn th√¨ b·ªè
            if attempt >= max_attempts:
                logger.error(f"safe_get: timeout qu√° {max_attempts} l·∫ßn, b·ªè url: {url}")
                return False

            # Retry (reset chrome)
            try:
                cleanup_driver()
                driver = get_driver()
                logger.info("ƒê√£ t·∫°o l·∫°i driver sau l·ªói timeout ƒë·ªÉ retry.")
            except Exception as e2:
                logger.error(f"Kh√¥ng t·∫°o l·∫°i ƒë∆∞·ª£c driver khi retry: {e2}")
                return False

            time.sleep(2)

    return False



# ======================== UTILITIES ========================
def to_absolute(base_url: str, href: str) -> str:
    return urljoin(base_url, href.strip())

def same_site(u: str, base_domain: str) -> bool:
    return urlparse(u).netloc.endswith(base_domain)

def contains_year(u: str) -> bool:
    return any(str(y) in u for y in range(2020, 2026))

def parse_datetime_maybe(s: str) -> str:
    if not s:
        return ""
    s = s.strip()
    fmts = [
        "%Y-%m-%dT%H:%M:%S%z", "%Y-%m-%dT%H:%M:%S",
        "%H:%M %d/%m/%Y", "%d/%m/%Y %H:%M", "%d-%m-%Y %H:%M", "%d/%m/%Y",
    ]
    for f in fmts:
        try:
            return datetime.strptime(s, f).isoformat()
        except:
            pass
    return s

def _local_midnight_epoch(d: date) -> int:
    dt = datetime(d.year, d.month, d.day, 0, 0, 0, tzinfo=VN_TZ)
    return int(dt.timestamp())

def _local_end_of_day_epoch(d: date) -> int:
    dt = datetime(d.year, d.month, d.day, 23, 59, 59, tzinfo=VN_TZ)
    return int(dt.timestamp())

def _date_from_str(s: str) -> date:
    return datetime.strptime(s, "%Y-%m-%d").date()

# ======================== URL BUILDER ========================
def build_day_url(cateid: str, from_epoch: int, to_epoch: int, page: Optional[int] = None) -> str:
    base = f"https://vnexpress.net/category/day/cateid/{cateid}/fromdate/{from_epoch}/todate/{to_epoch}/allcate/{cateid}"
    return base

def try_paginated_urls(base: str, page: int) -> List[str]:
    if page <= 1:
        return [base]
    return [f"{base}?page={page}", f"{base}?p={page}"]

# ======================== META & CONTENT ========================
def extract_common_meta(soup: BeautifulSoup, topic: str) -> Dict[str, str]:
    title = soup.find("h1").get_text(strip=True) if soup.find("h1") else ""
    desc = soup.find("meta", {"name": "description"})
    description = desc["content"].strip() if desc and desc.get("content") else ""

    keywords = []
    tag = soup.find("meta", {"name": "keywords"})
    if tag and tag.get("content"):
        keywords = [k.strip() for k in tag["content"].split(",")]

    author = ""
    for css in [".author", "p.author", "span.author", ".article__author"]:
        tag = soup.select_one(css)
        if tag:
            author = tag.get_text(strip=True)
            break

    publish_date = ""
    meta_tag = soup.find("meta", {"property": "article:published_time"})
    if meta_tag and meta_tag.get("content"):
        publish_date = parse_datetime_maybe(meta_tag["content"])

    if not publish_date:
        t = soup.select_one("time[datetime]")
        if t and (t.get("datetime") or t.get_text(strip=True)):
            publish_date = parse_datetime_maybe(t.get("datetime") or t.get_text(strip=True))

    if not publish_date:
        for css in [".date", "span.time", ".ArticleDateTime", ".breadcrumb_time"]:
            t = soup.select_one(css)
            if t:
                publish_date = parse_datetime_maybe(t.get_text(strip=True))
                if publish_date:
                    break

    image_url = ""
    og_img = soup.find("meta", {"property": "og:image"})
    if og_img and og_img.get("content"):
        image_url = og_img["content"]
    if not image_url:
        img_tag = soup.select_one("article.fck_detail img")
        if img_tag and img_tag.get("src"):
            image_url = img_tag["src"]

    sub_topic = extract_subtopic_vnexpress(soup, topic)
    return {
        "title": title,
        "description": description,
        "author": author,
        "publish_date": publish_date,
        "keywords": keywords,
        "topic": topic,
        "sub_topic": sub_topic,
        "image_url": image_url,
    }

def extract_content_vnexpress(soup: BeautifulSoup) -> str:
    body = soup.select_one("article.fck_detail")
    if body:
        paras = [p.get_text(" ", strip=True) for p in body.find_all("p")]
        return " ".join([p for p in paras if p])
    return ""

def extract_subtopic_vnexpress(soup: BeautifulSoup, topic: str) -> str:
    """
    Th·ª≠ l·∫•y subtopic t·ª´ breadcrumb.
    - Lo·∫°i b·ªè 'Trang ch·ªß' v√† topic cha ƒëang crawl.
    - Tr·∫£ v·ªÅ h·∫°ng m·ª•c con s√¢u nh·∫•t (cu·ªëi breadcrumb).
    """
    # C√°c selector breadcrumb kh·∫£ dƒ© tr√™n VnExpress
    crumbs = []
    for sel in ["ul.breadcrumb li a", "ul#breadcrumb li a", "nav.breadcrumb a"]:
        for a in soup.select(sel):
            txt = a.get_text(strip=True)
            if txt:
                crumbs.append(txt)

    if not crumbs:
        # Th·ª≠ l·∫•y t·ª´ meta
        meta_sec = soup.find("meta", {"property": "article:section"})
        if meta_sec and meta_sec.get("content"):
            sec = meta_sec["content"].strip()
            # N·∫øu section kh√¥ng tr√πng topic cha, coi nh∆∞ subtopic
            if topic and sec and sec.lower() != topic.lower():
                return sec
            return ""

    # Chu·∫©n h√≥a v√† l·ªçc
    bad = {"trang ch·ªß", "home", "video"}
    topic_l = (topic or "").strip().lower()
    candidates = [c for c in crumbs if c and c.strip().lower() not in bad]

    # Lo·∫°i breadcrumb tr√πng topic cha
    candidates = [c for c in candidates if c.strip().lower() != topic_l]

    # N·∫øu c√≤n nhi·ªÅu c·∫•p, l·∫•y c·∫•p s√¢u nh·∫•t (cu·ªëi danh s√°ch)
    if candidates:
        return candidates[-1].strip()

    return ""

def extract_references_vnexpress(soup: BeautifulSoup) -> List[str]:
    """
    T√¨m 'Ngu·ªìn/ Theo ...' trong b√†i vi·∫øt.
    Heuristic:
      - T√¨m c√°c 'p' ·ªü cu·ªëi b√†i c√≥ ch·ª©a t·ª´ kh√≥a 'Theo', 'Ngu·ªìn'.
      - T√¨m c√°c c·ª•m trong ngo·∫∑c (V√≠ d·ª•: '(Theo Reuters)').
    Tr·∫£ v·ªÅ danh s√°ch unique, gi·ªØ th·ª© t·ª± xu·∫•t hi·ªán.
    """
    body = soup.select_one("article.fck_detail") or soup.select_one("article") or soup
    texts = []

    # 1) C√°c 'p' cu·ªëi b√†i (∆∞u ti√™n 5 ƒëo·∫°n cu·ªëi)
    paras = body.find_all("p") if body else []
    for p in paras[-8:]:
        t = p.get_text(" ", strip=True)
        if not t:
            continue
        if re.search(r"(?i)\b(theo|ngu·ªìn)\b", t):
            texts.append(t)

    # 2) T√¨m trong to√†n vƒÉn c√°c m·∫´u '(Theo ...)', '(Ngu·ªìn: ...)'
    full_text = body.get_text(" ", strip=True) if body else ""
    paren_matches = re.findall(r"\((?:\s*(?:Theo|Ngu·ªìn)\s*[:\-]?\s*)([^)]+)\)", full_text, flags=re.IGNORECASE)
    for m in paren_matches:
        texts.append(m.strip())

    # 3) R√∫t ra t√™n ngu·ªìn t·ª´ c√°c d√≤ng thu th·∫≠p
    refs: List[str] = []
    for t in texts:
        # Bi·∫øn th·ªÉ: "Theo Reuters", "Theo AFP", "Ngu·ªìn: B·ªô Y t·∫ø", "Theo b√°o c√°o c·ªßa WHO"
        for pat in [
            r"(?i)\btheo\s+([A-Zƒê][\w\s&\-.‚Äì‚Äî,:]+)$",
            r"(?i)\bngu·ªìn[:\-]?\s+([A-Zƒê][\w\s&\-.‚Äì‚Äî,:]+)$",
            r"(?i)\btheo\s+([A-Zƒê][\w\s&\-.‚Äì‚Äî,:]+?),",  # l·∫•y t·ªõi d·∫•u ph·∫©y
            r"(?i)\bngu·ªìn[:\-]?\s+([A-Zƒê][\w\s&\-.‚Äì‚Äî,:]+?),",
        ]:
            m = re.search(pat, t)
            if m:
                cand = m.group(1).strip()
                # L√†m g·ªçn ph·∫ßn ƒëu√¥i r√°c
                cand = re.sub(r"[\s,.;:‚Äì‚Äî\-]+$", "", cand).strip()
                # Gi·ªõi h·∫°n ƒë·ªô d√†i h·ª£p l√Ω
                if 2 <= len(cand) <= 80:
                    refs.append(cand)

    # 4) Unique gi·ªØ th·ª© t·ª±
    seen: Set[str] = set()
    out: List[str] = []
    for r in refs:
        key = r.lower()
        if key not in seen:
            seen.add(key)
            out.append(r)
    return out

def extract_comments_from_soup(soup: BeautifulSoup, limit: int = 50) -> Tuple[int, List[Dict]]:
    """
    Parse comment t·ª´ HTML ƒë√£ load xong (ƒë√£ c√≥ #list_comment, #total_comment).
    KH√îNG d√πng Selenium trong h√†m n√†y ƒë·ªÉ tr√°nh timeout / retry.
    """
    comment_count = 0
    top_comments: List[Dict] = []

    try:
        label_tag = soup.find("label", id="total_comment")
        if not label_tag:
            return 0, []

        raw_text = label_tag.get_text(strip=True)
        if not raw_text or raw_text in ["", "Xem th√™m", "ƒêang t·∫£i..."]:
            return 0, []

        cleaned = (
            raw_text.lower()
            .replace("k", "000")
            .replace(".", "")
            .replace(",", "")
            .replace("(", "")
            .replace(")", "")
        )
        m = re.search(r"\d+", cleaned)
        if not m:
            return 0, []

        comment_count = int(m.group())
        if comment_count == 0:
            return 0, []

        comment_divs = soup.select("div#list_comment div.comment_item")[:limit]
        if not comment_divs:
            return comment_count, []

        for div in comment_divs:
            # --- T√™n ---
            name_tag = div.select_one("span.txt-name a.nickname") or div.select_one("span.txt-name")
            commenter_name = name_tag.get_text(strip=True) if name_tag else ""

            # --- N·ªôi dung ---
            content_tag = div.select_one("p.full_content") or div.select_one("p.content_less")
            content_text = content_tag.get_text(" ", strip=True) if content_tag else ""
            if commenter_name and content_text.startswith(commenter_name):
                content_text = content_text[len(commenter_name):].strip()

            # --- T·ªïng like ---
            like_tag = div.select_one("div.reactions-total a.number")
            total_likes = 0
            if like_tag:
                like_text = (
                    like_tag.get_text(strip=True)
                    .lower()
                    .replace("k", "000")
                    .replace(".", "")
                    .replace(",", "")
                )
                like_match = re.search(r"\d+", like_text)
                if like_match:
                    total_likes = int(like_match.group())

            # --- interaction_details ---
            interaction_details = {}
            detail_div = div.select_one("div.reactions-detail")
            if detail_div:
                for item in detail_div.select("div.item"):
                    img_tag = item.select_one("span.icons img")
                    label_vi = img_tag.get("alt", "").strip() if img_tag else ""
                    if not label_vi:
                        label_tag2 = item.find("span", class_="label")
                        if label_tag2:
                            label_vi = label_tag2.get_text(strip=True)
                    label_en = REACTION_MAP.get(label_vi, label_vi.lower().replace(" ", "_"))

                    num_tag = item.find("strong")
                    if label_en and num_tag:
                        num_text = (
                            num_tag.get_text(strip=True)
                            .lower()
                            .replace("k", "000")
                            .replace(".", "")
                            .replace(",", "")
                        )
                        num_match = re.search(r"\d+", num_text)
                        if num_match:
                            num = int(num_match.group())
                            if num > 0:
                                interaction_details[label_en] = num

            if commenter_name or content_text:
                top_comments.append(
                    {
                        "commenter_name": commenter_name,
                        "comment_content": content_text,
                        "total_likes": total_likes,
                        "interaction_details": interaction_details,
                    }
                )

    except Exception as e:
        logger.warning(f"L·ªói parse comment t·ª´ HTML: {e}")
        return 0, []

    return comment_count, top_comments


def wait_for_comment_count(driver, timeout=15):
    """
    ƒê·ª£i t·ªõi khi #total_comment c√≥ s·ªë, nh∆∞ng KH√îNG ch·ªù qu√° l√¢u.
    Kh√¥ng d√πng WebDriverWait ph·ª©c t·∫°p ƒë·ªÉ tr√°nh treo / retry ng·∫ßm.
    """
    start = time.time()
    while time.time() - start < timeout:
        try:
            el = driver.find_element(By.ID, "total_comment")
            txt = el.text.strip()
            if txt and txt.lower() not in ("xem th√™m", "ƒëang t·∫£i...", "ƒëang t·∫£i", "dang tai...", "dang tai"):
                if re.search(r"\d", txt):
                    return True
        except Exception:
            pass
        time.sleep(1)

    # H·∫øt th·ªùi gian m√† v·∫´n ch∆∞a c√≥ s·ªë ‚Üí coi nh∆∞ kh√¥ng c√≥ comment
    return False



# ======================== COMMENTS ========================
def extract_comments_vnexpress(driver, limit: int = 50) -> Tuple[int, List[Dict]]:
    """
    D√πng Selenium ƒë·ªÉ:
      - ƒê·ª£i ph·∫ßn comment xu·∫•t hi·ªán (nhanh, t·ªëi ƒëa ~15s)
      - Scroll t·ªõi ƒë√≥ cho JS load
    Sau ƒë√≥:
      - L·∫•y HTML m·ªôt l·∫ßn
      - Parse comment b·∫±ng BeautifulSoup (extract_comments_from_soup)
    N·∫øu b·∫•t c·ª© b∆∞·ªõc n√†o ch·∫≠m / l·ªói ‚Üí tr·∫£ v·ªÅ (0, []) lu√¥n, KH√îNG retry v√≤ng v√≤ng.
    """
    try:
        # 1) Th·ª≠ ƒë·ª£i ch√∫t cho total_comment c√≥ s·ªë
        ok = wait_for_comment_count(driver, timeout=15)
        if not ok:
            logger.info("Kh√¥ng c√≥ ho·∫∑c kh√¥ng load xong ph·∫ßn comment ‚Üí comment_count = 0")
            return 0, []

        # 2) Scroll ƒë·∫øn v√πng comment (n·∫øu c√≥)
        try:
            comment_section = driver.find_element(By.ID, "list_comment")
            driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", comment_section)
        except Exception:
            try:
                count_element = driver.find_element(By.ID, "total_comment")
                driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", count_element)
            except Exception:
                # Kh√¥ng scroll ƒë∆∞·ª£c c≈©ng k·ªá, d√πng HTML hi·ªán t·∫°i
                pass

        time.sleep(2)  # cho JS render th√™m ch√∫t

        # 3) L·∫•y HTML M·ªòT L·∫¶N, n·∫øu Selenium c√≥ v·∫•n ƒë·ªÅ th√¨ b·ªè qua lu√¥n
        try:
            html = driver.page_source
        except Exception as e:
            logger.warning(f"L·ªói l·∫•y page_source khi ƒë·ªçc comment: {e}")
            return 0, []

        soup = BeautifulSoup(html, "html.parser")
        return extract_comments_from_soup(soup, limit)

    except Exception as e:
        logger.warning(f"L·ªói l·∫•y comment (t·∫Øt nhanh, kh√¥ng retry): {e}")
        return 0, []


# ======================== HTTP ========================
_thread_local_http = threading.local()

def _get_session() -> requests.Session:
    s = getattr(_thread_local_http, "session", None)
    if s is None:
        s = requests.Session()
        s.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "vi,vi-VN;q=0.9,en;q=0.8",
        })
        _thread_local_http.session = s
    return s

def http_get(url: str, timeout=HTTP_TIMEOUT, retries=HTTP_RETRIES) -> Optional[str]:
    sess = _get_session()
    base_headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "Accept-Language": "vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7",
        "Connection": "keep-alive",
    }
    alt_uas = [
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    ]

    for i in range(retries + 1):
        try:
            r = sess.get(url, headers=base_headers, timeout=timeout)
            if r.status_code == 200 and r.text:
                return r.text
            elif r.status_code == 406:
                ua = alt_uas[min(i, len(alt_uas) - 1)]
                sess.headers.update({"User-Agent": ua})
                logger.warning(f"[HTTP] 406 -> ƒë·ªïi UA ({i+1}/{retries})")
            else:
                logger.warning(f"[HTTP] {r.status_code} for {url}")
        except Exception as e:
            logger.warning(f"[HTTP] err {e} ({i+1}/{retries})")
        time.sleep(0.4 * (i + 1))
    return None

# ======================== LINKS ========================
def looks_like_article(url: str, domain: str) -> bool:
    if exclude_re.search(url):
        return False
    if domain.endswith("vnexpress.net"):
        return url.endswith(".html") or contains_year(url)
    return contains_year(url)

def dedupe_preserve_order(xs: List[str]) -> List[str]:
    seen = set()
    return [x for x in xs if x not in seen and not seen.add(x)]

def get_links_from_listpage_http(list_url: str) -> List[str]:
    html = http_get(list_url)
    if not html:
        return []
    soup = BeautifulSoup(html, "html.parser")
    base = f"{urlparse(list_url).scheme}://{urlparse(list_url).netloc}"
    domain = urlparse(list_url).netloc
    links = []
    for a in soup.find_all("a", href=True):
        abs_url = to_absolute(base, a["href"])
        if same_site(abs_url, domain) and looks_like_article(abs_url, domain):
            links.append(abs_url)
    return dedupe_preserve_order(links)

def collect_links_in_daterange(cateid: str, d_from: date, d_to: date, max_pages: int = 20) -> List[str]:
    if d_to < d_from:
        raise ValueError("d_to ph·∫£i >= d_from")
    from_epoch = _local_midnight_epoch(d_from)
    to_epoch = _local_end_of_day_epoch(d_to)
    base = build_day_url(cateid, from_epoch, to_epoch)

    all_links = []
    for page in range(1, max_pages + 1):
        page_variants = try_paginated_urls(base, page)
        page_links = []
        loaded = False
        for u in page_variants:
            links = get_links_from_listpage_http(u)
            if links:
                page_links = links
                loaded = True
                logger.info(f"Loaded page {page} with {len(page_links)} links")
                break
        if not loaded:
            break
        before = len(all_links)
        all_links = dedupe_preserve_order(all_links + page_links)
        if len(all_links) == before:
            logger.info("H·∫øt link m·ªõi.")
            break
        time.sleep(PER_PAGE_SLEEP)
    return all_links

# ======================== PARSE ARTICLE ========================
def parse_article(url: str, topic: str, year: int, month: int, day: int) -> Dict:
    driver = get_driver()
    timeout = 30 if ENABLE_COMMENTS else 10
    if not safe_get(driver, url, wait_css="article.fck_detail, h1", timeout=timeout):
        logger.warning(f"Kh√¥ng t·∫£i ƒë∆∞·ª£c b√†i: {url}")
        return {}

    try:
        soup = BeautifulSoup(driver.page_source, "html.parser")
        meta = extract_common_meta(soup, topic)
        main_content = extract_content_vnexpress(soup)

        # author = meta["author"]
        # if not author:
        #     author_tag = soup.select_one("p.Normal[style*='text-align:right;'] strong")
        #     if author_tag:
        #         author = author_tag.get_text(strip=True)

        author = meta["author"]
        if not author:
            p_candidates = soup.select("article.fck_detail p.Normal") or soup.select("p.Normal")
            for p in reversed(p_candidates):
                align = (p.get("align") or "").lower()
                style = (p.get("style") or "").replace(" ", "").lower()
                if align == "right" or "text-align:right" in style or "text-align:center" in style:
                    strong = p.find("strong")
                    if strong:
                        author = strong.get_text(strip=True)
                        break
        meta["author"] = author

        references = extract_references_vnexpress(soup)

        comment_count, top_comments = (extract_comments_vnexpress(driver, COMMENT_LIMIT)
                                      if ENABLE_COMMENTS else (0, []))

        return {
            "title": meta["title"],
            "url": url,
            "description": meta["description"],
            "topic": meta["topic"],
            "sub_topic": meta["sub_topic"],
            "keywords": meta["keywords"],
            "publish_date": meta["publish_date"],
            "author": author,
            "image_url": meta["image_url"],
            "references": references,
            "main_content": main_content,
            "comment_count": comment_count,
            "top_comments": top_comments,
        }
    except Exception as e:
        logger.error(f"L·ªói parse b√†i {url}: {e}")
        return {}

# ======================== CRAWL BY DATE ========================
def crawl_vnexpress_by_date(topic_name: str, the_date: str, max_articles: Optional[int] = None) -> List[Dict]:
    if topic_name not in CATEIDS:
        raise ValueError(f"Ch∆∞a bi·∫øt cateid cho '{topic_name}'.")

    d = _date_from_str(the_date)
    y, m, dd = d.year, d.month, d.day

    cateid = CATEIDS[topic_name]
    links = collect_links_in_daterange(cateid, d, d)

    if max_articles:
        links = links[:max_articles]
    logger.info(f"[{topic_name}] {the_date} - {len(links)} links")

    items: List[Dict] = []

    driver = get_driver()
    articles_since_restart = 0
    RESTART_THRESHOLD = 20  # m·ªói 20 b√†i reset driver m·ªôt l·∫ßn 

    for url in links:
        try:
            dct = parse_article(url, topic_name, y, m, dd)
        except Exception as e:
            logger.error(f"[FATAL_URL] L·ªói kh√¥ng mong ƒë·ª£i khi x·ª≠ l√Ω URL {url}: {e}")
            continue

        if dct and (dct.get("title") or dct.get("main_content")):
            items.append(dct)
        else:
            logger.info(f"B·ªè qua b√†i r·ªóng ho·∫∑c parse kh√¥ng ƒë∆∞·ª£c: {url}")

        # === RESET DRIVER ƒê·ªäNH K·ª≤ ƒê·ªÇ GI·∫¢M RAM ===
        articles_since_restart += 1
        if articles_since_restart >= RESTART_THRESHOLD:
            logger.info(f"ƒê√£ x·ª≠ l√Ω {articles_since_restart} b√†i -> reset Chrome driver ƒë·ªÉ gi·∫£i ph√≥ng RAM.")
            cleanup_driver()
            driver = get_driver()
            articles_since_restart = 0

    if not items:
        return []

    # Ph·∫ßn s·∫Øp x·∫øp & l∆∞u GCS gi·ªØ nguy√™n
    items.sort(
        key=lambda x: (_dt_from_iso_local(x.get("publish_date")) or datetime.min),
        reverse=True,
    )

    results: List[Dict] = []
    for dct in items:
        ts = _dt_from_iso_local(dct.get("publish_date"))
        if ts:
            y, m, dd = ts.year, ts.month, ts.day
            prefix = ts.strftime("%Y%m%dT%H%M%S") + "_"
        else:
            prefix = ""
            if USE_INGESTED_DATE_FALLBACK:
                now = now_vn()
                y, m, dd = now.year, now.month, now.day

        dct["year"], dct["month"], dct["day"] = y, m, dd
        object_name = build_object_path(topic_name, y, m, dd, prefix=prefix)
        payload = json.dumps(dct, ensure_ascii=False).encode("utf-8")
        save_to_gcs(payload, object_name)
        results.append(dct)

    return results



def crawl_vnexpress_by_daterange(topic_name: str, date_from: str, date_to: Optional[str] = None,
                                 max_articles_per_day: Optional[int] = None) -> List[Dict]:
    d_start = _date_from_str(date_from)
    d_end = _date_from_str(date_to) if date_to else d_start
    cur = d_start
    all_items = []
    while cur <= d_end:
        day_str = cur.strftime("%Y-%m-%d")
        logger.info(f"[DAY] {topic_name} - {day_str}")

        try:
            day_items = crawl_vnexpress_by_date(
                topic_name,
                day_str,
                max_articles=max_articles_per_day
            )
        except Exception as e:
            # Ng√†y n√†y c√≥ v·∫•n ƒë·ªÅ (bug logic, v.v.) -> log r·ªìi b·ªè qua, kh√¥ng d·ª´ng ch∆∞∆°ng tr√¨nh
            logger.error(f"[FATAL_DAY] L·ªói khi crawl ng√†y {day_str} cho topic {topic_name}: {e}")
            day_items = []

        if day_items:
            all_items.extend(day_items)
        else:
            logger.info(f"[DAY] Kh√¥ng c√≥ item n√†o (ho·∫∑c l·ªói) cho {topic_name} - {day_str}")

        cur += timedelta(days=1)

    return all_items


# ======================== MAIN ========================
if __name__ == "__main__":
    # Healthcheck GCS
    save_to_gcs(b'{"ok": true}', "healthcheck/ok.json")
    logger.info("Smoke test GCS: OK")

    start_date = os.getenv("START_DATE", "2025-03-01")
    end_date = os.getenv("END_DATE", "2025-05-31")     


    # topics_env = os.getenv("TOPICS")
    # if topics_env:
    #     topic_list = [t.strip() for t in topics_env.split(",") if t.strip()]
    # else:
    #     topic_list = list(CATEIDS.keys())

    

    # L·ªçc topic 
    topics_env = os.getenv("TOPICS")
    if topics_env:
        topic_list = [t.strip() for t in topics_env.split(",") if t.strip()]
    else:
        excluded_topics = {"Th·∫ø gi·ªõi"}
        topic_list = [t for t in CATEIDS.keys() if t in excluded_topics]


    logger.info(f"[TOPICS] S·∫Ω crawl song song {len(topic_list)} topic: {topic_list}")
    logger.info(f"[DATE RANGE] {start_date} ‚Üí {end_date}")

    from concurrent.futures import ThreadPoolExecutor, as_completed

    # =============== FUNCTION ƒë·ªÉ ch·∫°y song song cho t·ª´ng topic ===============
    def crawl_one_topic(topic_name: str):
        try:
            logger.info(f"[START_TOPIC] {topic_name}")
            result = crawl_vnexpress_by_daterange(
                topic_name=topic_name,
                date_from=start_date,
                date_to=end_date,
                max_articles_per_day=None
            )
            logger.info(f"[DONE_TOPIC] {topic_name}: {len(result)} b√†i")
            return topic_name, result
        except Exception as e:
            logger.error(f"[ERR_TOPIC] {topic_name}: {e}")
            return topic_name, []
        finally:
            cleanup_driver()

    # =============== CH·∫†Y SONG SONG ===============
    MAX_TOPIC_WORKERS = min(MAX_WORKERS, len(topic_list))
    all_results = []

    with ThreadPoolExecutor(max_workers=MAX_TOPIC_WORKERS) as executor:
        future_map = {executor.submit(crawl_one_topic, topic): topic for topic in topic_list}

        for fut in as_completed(future_map):
            topic = future_map[fut]
            try:
                topic_name, items = fut.result()
                logger.info(f"[FINISHED] {topic_name}: {len(items)} b√†i")
                all_results.extend(items)
            except Exception as e:
                logger.error(f"[FATAL] Khi x·ª≠ l√Ω topic {topic}: {e}")

    logger.info("Ho√†n t·∫•t crawl to√†n b·ªô topic.")
    cleanup_driver()
    import atexit
    atexit.register(cleanup_driver)
